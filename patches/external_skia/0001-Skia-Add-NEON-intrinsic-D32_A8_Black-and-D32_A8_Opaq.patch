From b6883dedd51f9da603f846a77ea9529cd4fa9978 Mon Sep 17 00:00:00 2001
From: Lucas Crowthers <lucasc@codeaurora.org>
Date: Wed, 30 Oct 2013 17:44:47 -0400
Subject: [PATCH 1/7] Skia: Add NEON intrinsic D32_A8_Black and D32_A8_Opaque

If NEON support is indiciated via __ARM_HAVE_NEON flag then use
an optimized NEON intrinsics version of the D32_A8_Black and D32_A8_Opaque
masking function.

Change-Id: I20d2bf15515d960b73e635784a0daf29302e1f12
---
 src/core/SkBlitMask_D32.cpp | 351 ++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 351 insertions(+)

diff --git a/src/core/SkBlitMask_D32.cpp b/src/core/SkBlitMask_D32.cpp
index 594a274..c6ae849 100644
--- a/src/core/SkBlitMask_D32.cpp
+++ b/src/core/SkBlitMask_D32.cpp
@@ -9,6 +9,11 @@
 #include "SkColor.h"
 #include "SkColorPriv.h"
 
+#if defined(__ARM_HAVE_NEON) && defined(SK_CPU_LENDIAN)
+    #define SK_USE_NEON
+    #include <arm_neon.h>
+#endif
+
 static void D32_A8_Color(void* SK_RESTRICT dst, size_t dstRB,
                          const void* SK_RESTRICT maskPtr, size_t maskRB,
                          SkColor color, int width, int height) {
@@ -39,8 +44,200 @@ static void D32_A8_Opaque(void* SK_RESTRICT dst, size_t dstRB,
 
     maskRB -= width;
     dstRB -= (width << 2);
+
+#ifdef SK_USE_NEON
+#define UNROLL 8
+#define UNROLL_BY_2 4
+#define UNROLL_BY_4 2
     do {
         int w = width;
+        int count = 0;
+        //cache line is 64 bytes
+        __builtin_prefetch(mask);
+        __builtin_prefetch(device);
+
+        if( w>= UNROLL){
+            /*mask or alpha*/
+            uint16x8_t alpha_full;
+            uint32x4_t alpha_lo, alpha_hi;
+            uint32x4_t alpha_slo, alpha_shi; /*Saturated and scaled */
+            uint32x4_t rb,ag;    /*For device premultiply */
+            uint32x4_t rbp,agp;  /*For pmc premultiply */
+            uint32x4_t dev_lo, dev_hi;
+            uint32x4_t pmc_dup = vdupq_n_u32(pmc);
+            uint32x4_t pmc_lo, pmc_hi;
+
+            do{
+                if( (int) count % 64 == 0){
+                   __builtin_prefetch(mask);
+                }
+                if( (int) count % 16 == 0){
+                   __builtin_prefetch(device);
+                }
+
+                alpha_full = vmovl_u8(vld1_u8(mask));
+                alpha_lo = vmovl_u16(vget_low_u16(alpha_full));
+                alpha_hi = vmovl_u16(vget_high_u16(alpha_full));
+
+                dev_lo = vld1q_u32(device);
+                dev_hi = vld1q_u32(device + 4);
+
+                /*SkAlpha255To256(255-aa)*/
+                //alpha_slo = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_lo), vdupq_n_u32(0x00000001));
+                alpha_slo = vsubq_u32( vdupq_n_u32(0x00000100), alpha_lo);
+                //alpha_shi = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_hi), vdupq_n_u32(0x00000001));
+                alpha_shi = vsubq_u32( vdupq_n_u32(0x00000100), alpha_hi);
+                /*SkAlpha255To256(aa)*/
+                alpha_lo = vaddq_u32(alpha_lo, vdupq_n_u32(0x00000001));
+                alpha_hi = vaddq_u32(alpha_hi, vdupq_n_u32(0x00000001));
+
+                rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_lo, vdupq_n_u32(0x00FF00FF)), alpha_slo), 8);
+                ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_lo, 8), vdupq_n_u32(0x00FF00FF)), alpha_slo);
+                dev_lo = vorrq_u32( vandq_u32(rb,  vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+                rbp = vshrq_n_u32( vmulq_u32( vandq_u32( pmc_dup, vdupq_n_u32(0x00FF00FF)), alpha_lo), 8);
+                agp = vmulq_u32( vandq_u32( vshrq_n_u32(pmc_dup, 8), vdupq_n_u32(0x00FF00FF)), alpha_lo);
+                pmc_lo = vorrq_u32( vandq_u32(rbp,  vdupq_n_u32(0x00FF00FF)), vandq_u32(agp, vdupq_n_u32(0xFF00FF00)));
+
+                dev_lo = vaddq_u32 ( pmc_lo, dev_lo);
+
+                rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_hi, vdupq_n_u32(0x00FF00FF)), alpha_shi), 8);
+                ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_hi, 8), vdupq_n_u32(0x00FF00FF)), alpha_shi);
+                dev_hi = vorrq_u32( vandq_u32(rb,	vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+                rbp = vshrq_n_u32( vmulq_u32( vandq_u32( pmc_dup, vdupq_n_u32(0x00FF00FF)), alpha_hi), 8);
+                agp = vmulq_u32( vandq_u32( vshrq_n_u32(pmc_dup, 8), vdupq_n_u32(0x00FF00FF)), alpha_hi);
+                pmc_hi = vorrq_u32( vandq_u32(rbp,  vdupq_n_u32(0x00FF00FF)), vandq_u32(agp, vdupq_n_u32(0xFF00FF00)));
+
+                dev_hi = vaddq_u32 ( pmc_hi, dev_hi);
+
+                vst1q_u32(device, dev_lo);
+                vst1q_u32(device + 4, dev_hi);
+
+                device += UNROLL;
+                mask += UNROLL;
+                count += UNROLL;
+                w -= UNROLL;
+               }while (w >= UNROLL);
+            }            else if(w >= UNROLL_BY_2){
+               /*mask or alpha*/
+               uint16x8_t alpha_full = vmovl_u8(vld1_u8(mask));
+               uint32x4_t alpha_lo;
+               uint32x4_t alpha_slo; /*Saturated and scaled */
+               uint32x4_t rb,ag;     /*For device premultiply */
+               uint32x4_t rbp,agp;   /*For pmc premultiply */
+               uint32x4_t dev_lo;
+               uint32x4_t pmc_lo;
+               uint32x4_t pmc_dup = vdupq_n_u32(pmc);
+
+               dev_lo = vld1q_u32(device);
+               alpha_lo = vmovl_u16(vget_low_u16(alpha_full));
+
+               /*SkAlpha255To256(255-aa)*/
+               //alpha_slo = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_lo), vdupq_n_u32(0x00000001));
+               alpha_slo = vsubq_u32( vdupq_n_u32(0x00000100), alpha_lo);
+               /*SkAlpha255To256(aa)*/
+               alpha_lo = vaddq_u32(alpha_lo, vdupq_n_u32(0x00000001));
+
+               rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_lo, vdupq_n_u32(0x00FF00FF)), alpha_slo), 8);
+               ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_lo, 8), vdupq_n_u32(0x00FF00FF)), alpha_slo);
+               dev_lo = vorrq_u32( vandq_u32(rb,  vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+               rbp = vshrq_n_u32( vmulq_u32( vandq_u32( pmc_dup, vdupq_n_u32(0x00FF00FF)), alpha_lo), 8);
+               agp = vmulq_u32( vandq_u32( vshrq_n_u32(pmc_dup, 8), vdupq_n_u32(0x00FF00FF)), alpha_lo);
+               pmc_lo = vorrq_u32( vandq_u32(rbp,  vdupq_n_u32(0x00FF00FF)), vandq_u32(agp, vdupq_n_u32(0xFF00FF00)));
+
+               dev_lo = vaddq_u32 ( pmc_lo, dev_lo);
+
+               vst1q_u32(device, dev_lo);
+
+               device += UNROLL_BY_2;
+               mask += UNROLL_BY_2;
+               w -= UNROLL_BY_2;
+
+               if (w >= UNROLL_BY_4){
+                   /*mask or alpha*/
+                   uint32x2_t alpha_lo;
+                   uint32x2_t alpha_slo; /*Saturated and scaled */
+                   uint32x2_t rb,ag;
+                   uint32x2_t rbp,agp;   /*For pmc premultiply */
+                   uint32x2_t dev_lo;
+                   uint32x2_t pmc_lo;
+                   uint32x2_t pmc_dup = vdup_n_u32(pmc);
+
+                   dev_lo = vld1_u32(device);
+                   alpha_lo = vget_low_u32(vmovl_u16(vget_high_u16(alpha_full)));
+
+                   /*SkAlpha255To256(255-aa)*/
+                   //alpha_slo = vadd_u32(vsub_u32( vdup_n_u32(0x000000FF), alpha_lo), vdup_n_u32(0x00000001));
+                   alpha_slo = vsub_u32( vdup_n_u32(0x00000100), alpha_lo);
+                   /*SkAlpha255To256(aa)*/
+                   alpha_lo = vadd_u32(alpha_lo, vdup_n_u32(0x00000001));
+
+                   rb = vshr_n_u32( vmul_u32( vand_u32( dev_lo, vdup_n_u32(0x00FF00FF)), alpha_slo), 8);
+                   ag = vmul_u32( vand_u32( vshr_n_u32(dev_lo, 8), vdup_n_u32(0x00FF00FF)), alpha_slo);
+                   dev_lo = vorr_u32( vand_u32(rb,  vdup_n_u32(0x00FF00FF)), vand_u32(ag, vdup_n_u32(0xFF00FF00)));
+                   rbp = vshr_n_u32( vmul_u32( vand_u32( pmc_dup, vdup_n_u32(0x00FF00FF)), alpha_lo), 8);
+                   agp = vmul_u32( vand_u32( vshr_n_u32(pmc_dup, 8), vdup_n_u32(0x00FF00FF)), alpha_lo);
+                   pmc_lo = vorr_u32( vand_u32(rbp,  vdup_n_u32(0x00FF00FF)), vand_u32(agp, vdup_n_u32(0xFF00FF00)));
+
+                   dev_lo = vadd_u32 ( pmc_lo, dev_lo);
+
+                   vst1_u32(device, dev_lo);
+
+                   device += UNROLL_BY_4;
+                   mask += UNROLL_BY_4;
+                   w -= UNROLL_BY_4;
+               }
+
+        } else if(w >= UNROLL_BY_4){
+                 /*mask or alpha*/
+                uint16x8_t alpha_full = vmovl_u8(vld1_u8(mask));
+                uint32x2_t alpha_lo;
+                uint32x2_t alpha_slo; /*Saturated and scaled */
+                uint32x2_t rb,ag;
+                uint32x2_t rbp,agp;   /*For pmc premultiply */
+                uint32x2_t dev_lo;
+                uint32x2_t pmc_lo;
+                uint32x2_t pmc_dup = vdup_n_u32(pmc);
+
+                dev_lo = vld1_u32(device);
+                alpha_lo = vget_low_u32(vmovl_u16(vget_low_u16(alpha_full)));
+
+                /*SkAlpha255To256(255-aa)*/
+                //alpha_slo = vadd_u32(vsub_u32( vdup_n_u32(0x000000FF), alpha_lo), vdup_n_u32(0x00000001));
+                alpha_slo = vsub_u32( vdup_n_u32(0x00000100), alpha_lo);
+                /*SkAlpha255To256(aa)*/
+                alpha_lo = vadd_u32(alpha_lo, vdup_n_u32(0x00000001));
+
+                rb = vshr_n_u32( vmul_u32( vand_u32( dev_lo, vdup_n_u32(0x00FF00FF)), alpha_slo), 8);
+                ag = vmul_u32( vand_u32( vshr_n_u32(dev_lo, 8), vdup_n_u32(0x00FF00FF)), alpha_slo);
+                dev_lo = vorr_u32( vand_u32(rb,  vdup_n_u32(0x00FF00FF)), vand_u32(ag, vdup_n_u32(0xFF00FF00)));
+                rbp = vshr_n_u32( vmul_u32( vand_u32( pmc_dup, vdup_n_u32(0x00FF00FF)), alpha_lo), 8);
+                agp = vmul_u32( vand_u32( vshr_n_u32(pmc_dup, 8), vdup_n_u32(0x00FF00FF)), alpha_lo);
+                pmc_lo = vorr_u32( vand_u32(rbp,  vdup_n_u32(0x00FF00FF)), vand_u32(agp, vdup_n_u32(0xFF00FF00)));
+
+                dev_lo = vadd_u32 ( pmc_lo, dev_lo);
+
+                vst1_u32(device, dev_lo);
+
+                device += UNROLL_BY_4;
+                mask += UNROLL_BY_4;
+                w -= UNROLL_BY_4;
+        }
+
+        /*residuals (which is everything that cannot be handled by neon) */
+        while( w > 0){
+            unsigned aa = *mask++;
+            if( (aa != 0) || (aa != 255)){
+                *device = SkAlphaMulQ(pmc, SkAlpha255To256(aa)) + SkAlphaMulQ(*device, SkAlpha255To256(255 - aa));
+            }
+            device += 1;
+            --w;
+        }
+        device = (uint32_t*)((char*)device + dstRB);
+        mask += maskRB;
+    } while (--height != 0);
+#else
+    do{
+        int w = width;
         do {
             unsigned aa = *mask++;
             *device = SkAlphaMulQ(pmc, SkAlpha255To256(aa)) + SkAlphaMulQ(*device, SkAlpha255To256(255 - aa));
@@ -49,6 +246,7 @@ static void D32_A8_Opaque(void* SK_RESTRICT dst, size_t dstRB,
         device = (uint32_t*)((char*)device + dstRB);
         mask += maskRB;
     } while (--height != 0);
+#endif
 }
 
 static void D32_A8_Black(void* SK_RESTRICT dst, size_t dstRB,
@@ -59,8 +257,160 @@ static void D32_A8_Black(void* SK_RESTRICT dst, size_t dstRB,
 
     maskRB -= width;
     dstRB -= (width << 2);
+
+#ifdef SK_USE_NEON
+#define UNROLL 8
+#define UNROLL_BY_2 4
+#define UNROLL_BY_4 2
     do {
         int w = width;
+        int count = 0;
+        //cache line is 64 bytes
+        __builtin_prefetch(mask);
+        __builtin_prefetch(device);
+
+        if( w>= UNROLL){
+            /*mask or alpha*/
+            uint16x8_t alpha_full;
+            uint32x4_t alpha_lo, alpha_hi;
+            uint32x4_t alpha_slo, alpha_shi; /*Saturated and scaled */
+            uint32x4_t rb,ag;
+            uint32x4_t dev_lo, dev_hi;
+
+            do{
+                if( (int) count % 64 == 0){
+                   __builtin_prefetch(mask);
+                }
+                if( (int) count % 16 == 0){
+                   __builtin_prefetch(device);
+                }
+
+                alpha_full = vmovl_u8(vld1_u8(mask));
+                alpha_lo = vmovl_u16(vget_low_u16(alpha_full));
+                alpha_hi = vmovl_u16(vget_high_u16(alpha_full));
+
+                dev_lo = vld1q_u32(device);
+                dev_hi = vld1q_u32(device + 4);
+
+                /*SkAlpha255To256(255-aa)*/
+                //alpha_slo = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_lo), vdupq_n_u32(0x00000001));
+                alpha_slo = vsubq_u32( vdupq_n_u32(0x00000100), alpha_lo);
+                //alpha_shi = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_hi), vdupq_n_u32(0x00000001));
+                alpha_shi = vsubq_u32( vdupq_n_u32(0x00000100), alpha_hi);
+
+                rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_lo, vdupq_n_u32(0x00FF00FF)), alpha_slo), 8);
+                ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_lo, 8), vdupq_n_u32(0x00FF00FF)), alpha_slo);
+                dev_lo = vorrq_u32( vandq_u32(rb,  vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+                dev_lo = vaddq_u32 ( vshlq_n_u32(alpha_lo, SK_A32_SHIFT), dev_lo);
+
+                rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_hi, vdupq_n_u32(0x00FF00FF)), alpha_shi), 8);
+                ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_hi, 8), vdupq_n_u32(0x00FF00FF)), alpha_shi);
+                dev_hi = vorrq_u32( vandq_u32(rb,	vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+                dev_hi = vaddq_u32( vshlq_n_u32(alpha_hi, SK_A32_SHIFT), dev_hi);
+
+                vst1q_u32(device, dev_lo);
+                vst1q_u32(device + 4, dev_hi);
+
+                device += UNROLL;
+                mask += UNROLL;
+                count += UNROLL;
+                w -= UNROLL;
+               }while (w >= UNROLL);
+            }            else if(w >= UNROLL_BY_2){
+               /*mask or alpha*/
+               uint16x8_t alpha_full = vmovl_u8(vld1_u8(mask));
+               uint32x4_t alpha_lo;
+               uint32x4_t alpha_slo; /*Saturated and scaled */
+               uint32x4_t rb,ag;
+               uint32x4_t dev_lo;
+
+               dev_lo = vld1q_u32(device);
+               alpha_lo = vmovl_u16(vget_low_u16(alpha_full));
+
+               /*SkAlpha255To256(255-aa)*/
+               //alpha_slo = vaddq_u32(vsubq_u32( vdupq_n_u32(0x000000FF), alpha_lo), vdupq_n_u32(0x00000001));
+               alpha_slo = vsubq_u32( vdupq_n_u32(0x00000100), alpha_lo);
+
+               rb = vshrq_n_u32( vmulq_u32( vandq_u32( dev_lo, vdupq_n_u32(0x00FF00FF)), alpha_slo), 8);
+               ag = vmulq_u32( vandq_u32( vshrq_n_u32(dev_lo, 8), vdupq_n_u32(0x00FF00FF)), alpha_slo);
+               dev_lo = vorrq_u32( vandq_u32(rb,  vdupq_n_u32(0x00FF00FF)), vandq_u32(ag, vdupq_n_u32(0xFF00FF00)));
+               dev_lo = vaddq_u32( vshlq_n_u32(alpha_lo, SK_A32_SHIFT), dev_lo);
+
+               vst1q_u32(device, dev_lo);
+
+               device += UNROLL_BY_2;
+               mask += UNROLL_BY_2;
+               w -= UNROLL_BY_2;
+
+               if (w >= UNROLL_BY_4){
+                   /*mask or alpha*/
+                   uint32x2_t alpha_lo;
+                   uint32x2_t alpha_slo; /*Saturated and scaled */
+                   uint32x2_t rb,ag;
+                   uint32x2_t dev_lo;
+
+                   dev_lo = vld1_u32(device);
+                   alpha_lo = vget_low_u32(vmovl_u16(vget_high_u16(alpha_full)));
+
+                   /*SkAlpha255To256(255-aa)*/
+                   //alpha_slo = vadd_u32(vsubq_u32( vdup_n_u32(0x000000FF), alpha_lo), vdup_n_u32(0x00000001));
+                   alpha_slo = vsub_u32( vdup_n_u32(0x00000100), alpha_lo);
+
+                   rb = vshr_n_u32( vmul_u32( vand_u32( dev_lo, vdup_n_u32(0x00FF00FF)), alpha_slo), 8);
+                   ag = vmul_u32( vand_u32( vshr_n_u32(dev_lo, 8), vdup_n_u32(0x00FF00FF)), alpha_slo);
+                   dev_lo = vorr_u32( vand_u32(rb,  vdup_n_u32(0x00FF00FF)), vand_u32(ag, vdup_n_u32(0xFF00FF00)));
+                   dev_lo = vadd_u32 ( vshl_n_u32(alpha_lo, SK_A32_SHIFT), dev_lo);
+
+                   vst1_u32(device, dev_lo);
+
+                   device += UNROLL_BY_4;
+                   mask += UNROLL_BY_4;
+                   w -= UNROLL_BY_4;
+               }
+        }else if(w >= UNROLL_BY_4){
+                /*mask or alpha*/
+                uint16x8_t alpha_full = vmovl_u8(vld1_u8(mask));
+                uint32x2_t alpha_lo;
+                uint32x2_t alpha_slo; /*Saturated and scaled */
+                uint32x2_t rb,ag;
+                uint32x2_t dev_lo;
+
+                dev_lo = vld1_u32(device);
+                alpha_lo = vget_low_u32(vmovl_u16(vget_low_u16(alpha_full)));
+
+                /*SkAlpha255To256(255-aa)*/
+                //alpha_slo = vadd_u32(vsub_u32( vdup_n_u32(0x000000FF), alpha_lo), vdup_n_u32(0x00000001));
+                alpha_slo = vsub_u32( vdup_n_u32(0x00000100), alpha_lo);
+
+                rb = vshr_n_u32( vmul_u32( vand_u32( dev_lo, vdup_n_u32(0x00FF00FF)), alpha_slo), 8);
+                ag = vmul_u32( vand_u32( vshr_n_u32(dev_lo, 8), vdup_n_u32(0x00FF00FF)), alpha_slo);
+                dev_lo = vorr_u32( vand_u32(rb,  vdup_n_u32(0x00FF00FF)), vand_u32(ag, vdup_n_u32(0xFF00FF00)));
+                dev_lo = vadd_u32 ( vshl_n_u32(alpha_lo, SK_A32_SHIFT), dev_lo);
+
+                vst1_u32(device, dev_lo);
+
+                device += UNROLL_BY_4;
+                mask += UNROLL_BY_4;
+                w -= UNROLL_BY_4;
+        }
+
+        /*residuals (which is everything that cannot be handled by neon) */
+        while( w > 0){
+            unsigned aa = *mask++;
+            if(aa == 255){
+              *device =  0xFF000000;
+            }else if (aa != 0){
+              *device = (aa << SK_A32_SHIFT) + SkAlphaMulQ(*device, SkAlpha255To256(255 - aa));
+            }
+            device += 1;
+            --w;
+        }
+        device = (uint32_t*)((char*)device + dstRB);
+        mask += maskRB;
+    } while (--height != 0);
+#else
+    do{
+        int w = width;
         do {
             unsigned aa = *mask++;
             *device = (aa << SK_A32_SHIFT) + SkAlphaMulQ(*device, SkAlpha255To256(255 - aa));
@@ -69,6 +419,7 @@ static void D32_A8_Black(void* SK_RESTRICT dst, size_t dstRB,
         device = (uint32_t*)((char*)device + dstRB);
         mask += maskRB;
     } while (--height != 0);
+#endif
 }
 
 SkBlitMask::BlitLCD16RowProc SkBlitMask::BlitLCD16RowFactory(bool isOpaque) {
-- 
2.6.2

